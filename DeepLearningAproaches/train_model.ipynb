{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R3Ptov7McoQ"
      },
      "source": [
        "Connect to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3kkOb3sfL0V"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DonAjUYMZhm"
      },
      "source": [
        "Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdVKRdOaiCXD"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print('Using PyTorch version', torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ-XUOD7M0J6"
      },
      "source": [
        "**Redesign DataSet for test data and train dataset**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6XsySeqiKWq"
      },
      "source": [
        "class_names = ['0', '1', '2']\n",
        "root_dir = '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray'\n",
        "source_dirs = ['0', '1', '2']\n",
        "\n",
        "if os.path.isdir(os.path.join(root_dir, source_dirs[1])):\n",
        "    os.mkdir(os.path.join(root_dir, 'test'))\n",
        "\n",
        "    for i, d in enumerate(source_dirs):\n",
        "        os.rename(os.path.join(root_dir, d), os.path.join(root_dir, class_names[i]))\n",
        "\n",
        "    for c in class_names:\n",
        "        os.mkdir(os.path.join(root_dir, 'test', c))\n",
        "\n",
        "    for c in class_names:\n",
        "        images = [x for x in os.listdir(os.path.join(root_dir, c)) ]\n",
        "        selected_images = random.sample(images, 111)\n",
        "        for image in selected_images:\n",
        "            source_path = os.path.join(root_dir, c, image)\n",
        "            target_path = os.path.join(root_dir, 'test', c, image)\n",
        "            shutil.move(source_path, target_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1dUXnvE2I0N"
      },
      "source": [
        "class ChestXRayDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dirs, transform):\n",
        "        def get_images(class_name):\n",
        "            images = [x for x in os.listdir(image_dirs[class_name]) ]\n",
        "            print(f'Found {len(images)} {class_name} examples')\n",
        "            return images\n",
        "        \n",
        "        self.images = {}\n",
        "        self.class_names = ['normal', 'pneumonia', 'covid']\n",
        "            \n",
        "        for c in self.class_names:\n",
        "            self.images[c] = get_images(c)\n",
        "            \n",
        "        self.image_dirs = image_dirs\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return sum([len(self.images[c]) for c in self.class_names])\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        class_name = random.choice(self.class_names)\n",
        "        index = index % len(self.images[class_name])\n",
        "        image_name = self.images[class_name][index]\n",
        "        image_path = os.path.join(self.image_dirs[class_name], image_name)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        return self.transform(image), self.class_names.index(class_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bpxqt1uNHjQ"
      },
      "source": [
        "# ***Data Loader and Normalization, GAN ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r56o2bujqY8"
      },
      "source": [
        "train_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(size=(224, 224)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224,0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cISIiY5TEbdN"
      },
      "source": [
        "test_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(size=(224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224,0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6g1LCPwEnau"
      },
      "source": [
        "train_dirs = {\n",
        "    'normal': '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray/0',\n",
        "    'pneumonia': '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray/1',\n",
        "    'covid': '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray/2'\n",
        "}\n",
        "train_dataset = ChestXRayDataset(train_dirs, train_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIn1uREtGsiW"
      },
      "source": [
        "\n",
        "test_dirs = {\n",
        "    'normal': '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray/test/0',\n",
        "    'pneumonia': '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray/test/1',\n",
        "    'covid': '/content/drive/My Drive/Colab Notebooks/GAN_Images/Xray/test/2'\n",
        "}\n",
        "test_dataset = ChestXRayDataset(test_dirs, test_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34J3aMDUGcSQ"
      },
      "source": [
        "batch_size = 5\n",
        "\n",
        "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                      shuffle=True)\n",
        "dl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                     shuffle=True)\n",
        "\n",
        "print('Num of training batches', len(dl_train))\n",
        "print('Num of test batches', len(dl_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud3jEQKxHbm9"
      },
      "source": [
        "class_names = train_dataset.class_names\n",
        "\n",
        "def show_images(images, labels, preds):\n",
        "    plt.figure(figsize=(12,12))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(1, 5, i +1, xticks=[], yticks=[])\n",
        "        image = image.numpy().transpose(1, 2, 0)\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = image * std + mean\n",
        "        image = np.clip(image, 0., 1.)\n",
        "        plt.imshow(image)\n",
        "        \n",
        "        col = 'green' if preds[i] == labels[i] else 'red'\n",
        "        \n",
        "        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n",
        "        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDp98d94j4Wm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hpVJngqH3_Y"
      },
      "source": [
        "images, labels = next(iter(dl_train))\n",
        "show_images(images, labels, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAnJl5PMIAjY"
      },
      "source": [
        "\n",
        "images, labels = next(iter(dl_test))\n",
        "show_images(images, labels, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo5yfOz1N8Ff"
      },
      "source": [
        "## **Residual Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUFS7nq1IAhh"
      },
      "source": [
        "\n",
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "print(resnet18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYjD1IVFj6Kw"
      },
      "source": [
        "\n",
        "resnet18.fc = torch.nn.Linear(in_features=512, out_features=3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet18.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3uVX9uf5z-r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsDJhA_DkBoG"
      },
      "source": [
        "def show_preds():\n",
        "    resnet18.eval()\n",
        "    images, labels = next(iter(dl_test))\n",
        "    outputs = resnet18(images)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    show_images(images, labels, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_OkbtkIl28S"
      },
      "source": [
        "show_preds()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsg_legQ-dXQ"
      },
      "source": [
        "def train(epochs):\n",
        "    print('Starting training..')\n",
        "    for e in range(epochs):\n",
        "        print('='*20)\n",
        "        print(f'Starting epoch {e + 1}/{epochs}')\n",
        "        print('='*20)\n",
        "        \n",
        "        train_loss = 0\n",
        "        \n",
        "        resnet18.train()\n",
        "        \n",
        "        for train_step, (images, labels) in enumerate(dl_train):\n",
        "            optimizer.zero_grad()\n",
        "           # images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = resnet18(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            if train_step % 20 == 0:\n",
        "                print('Evaluating at step', train_step)\n",
        "                acc = 0\n",
        "                val_loss = 0\n",
        "                resnet18.eval()\n",
        "            \n",
        "                for val_step, (images, labels) in enumerate(dl_test):\n",
        "                    outputs = resnet18(images)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "                    val_loss +=loss.item()\n",
        "                \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    acc += sum((preds == labels).numpy())\n",
        "                val_loss /= (val_step +1)\n",
        "                acc = acc / len(test_dataset)\n",
        "                print(f'Val loss: {val_loss:.4f}, Acc: {acc:.4f}')\n",
        "                show_preds()\n",
        "                  \n",
        "                resnet18.train()\n",
        "                  \n",
        "                if acc > 0.95:\n",
        "                    print('Performance condition satisfied')\n",
        "  \n",
        "                    break\n",
        "        train_loss /= (train_step + 1)\n",
        "        print(f'Training loss: {train_loss:.4f}')\n",
        "    e=e+1\n",
        "    if e>3:\n",
        "      return        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC-NgQJmm1O2"
      },
      "source": [
        "hist=train(epochs=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwIhgKsipUD"
      },
      "source": [
        "def show_preds():\n",
        "    resnet18.eval()\n",
        "    images, labels = next(iter(dl_test))\n",
        "    outputs = resnet18(images)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    show_images(images, labels, preds)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kszWe17kizy-"
      },
      "source": [
        "show_preds()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJTwXIBdO74d"
      },
      "source": [
        "# **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3NWfijTf8En"
      },
      "source": [
        "nb_classes = 3\n",
        "\n",
        "confusion_matrix1 = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dl_test):\n",
        "        resnet18.eval()\n",
        "        outputs = resnet18(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "                confusion_matrix1[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_4twJVl2O2"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    precision = confusion_matrix1[1,1] / sum(confusion_matrix1[:,1])\n",
        "    recall    = confusion_matrix1[1,1] / sum(confusion_matrix1[1,:])\n",
        "    f1_score  = 2*precision*recall / (precision + recall)\n",
        "    stats_text = \"\\n\\n\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(precision,recall,f1_score)\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' \n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label' + stats_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5CK4BEXos5G"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr8Uj_3nsocq"
      },
      "source": [
        "plot_confusion_matrix(confusion_matrix1, train_dataset.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcjEZr923-3P"
      },
      "source": [
        "\n",
        "precision = confusion_matrix1[1,1] / sum(confusion_matrix1[:,1])\n",
        "recall    = confusion_matrix1[1,1] / sum(confusion_matrix1[1,:])\n",
        "f1_score  = 2*precision*recall / (precision + recall)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggC2hV865eiK"
      },
      "source": [
        "print(precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjNGIAnU1_Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNhFfSeCPFvG"
      },
      "source": [
        "# **Residual Networ for 34 layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBolUh3qU3KP"
      },
      "source": [
        "\n",
        "resnet34 = torchvision.models.resnet34(pretrained=True)\n",
        "print(resnet34)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aezD72W0WGL6"
      },
      "source": [
        "\n",
        "resnet34.fc = torch.nn.Linear(in_features=512, out_features=3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet34.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRozA4WRWa6G"
      },
      "source": [
        "def train(epochs):\n",
        "    print('Starting training..')\n",
        "    for e in range(epochs):\n",
        "        print('='*20)\n",
        "        print(f'Starting epoch {e + 1}/{epochs}')\n",
        "        print('='*20)\n",
        "        \n",
        "        train_loss = 0\n",
        "        \n",
        "        resnet34.train()\n",
        "        \n",
        "        for train_step, (images, labels) in enumerate(dl_train):\n",
        "            optimizer.zero_grad()\n",
        "           # images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = resnet34(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            if train_step % 20 == 0:\n",
        "                print('Evaluating at step', train_step)\n",
        "                acc = 0\n",
        "                val_loss = 0\n",
        "                resnet34.eval()\n",
        "            \n",
        "                for val_step, (images, labels) in enumerate(dl_test):\n",
        "                    outputs = resnet34(images)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "                    val_loss +=loss.item()\n",
        "                \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    acc += sum((preds == labels).numpy())\n",
        "                val_loss /= (val_step +1)\n",
        "                acc = acc / len(test_dataset)\n",
        "                print(f'Val loss: {val_loss:.4f}, Acc: {acc:.4f}')\n",
        "                show_preds()\n",
        "                  \n",
        "                resnet34.train()\n",
        "                  \n",
        "                if acc > 0.95:\n",
        "                    print('Performance condition satisfied')\n",
        "  \n",
        "                    break\n",
        "        train_loss /= (train_step + 1)\n",
        "        print(f'Training loss: {train_loss:.4f}')\n",
        "    e=e+1\n",
        "    if e>3:\n",
        "      return        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFjouBOXajG"
      },
      "source": [
        "hist=train(epochs=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EnTXzvqXj0h"
      },
      "source": [
        "nb_classes = 3\n",
        "\n",
        "confusion_matrix2 = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dl_test):\n",
        "\n",
        "        resnet34.eval()\n",
        "        outputs = resnet34(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "                confusion_matrix2[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tJAfp1jlL-s"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix2(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    precision = confusion_matrix2[1,1] / sum(confusion_matrix2[:,1])\n",
        "    recall    = confusion_matrix2[1,1] / sum(confusion_matrix2[1,:])\n",
        "    f1_score  = 2*precision*recall / (precision + recall)\n",
        "    stats_text = \"\\n\\n\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(precision,recall,f1_score)\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' \n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label' + stats_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul3_hV7PYSKE"
      },
      "source": [
        "plot_confusion_matrix2(confusion_matrix2, train_dataset.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i2kHnXIaZ5x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUFpaP12YfPx"
      },
      "source": [
        "\n",
        "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
        "print(resnet50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6bq-eGaYf4L"
      },
      "source": [
        "\n",
        "resnet50.fc = torch.nn.Linear(in_features=2048, out_features=3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet50.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjZBQrVeZ-8K"
      },
      "source": [
        "def train(epochs):\n",
        "    print('Starting training..')\n",
        "    for e in range(epochs):\n",
        "        print('='*20)\n",
        "        print(f'Starting epoch {e + 1}/{epochs}')\n",
        "        print('='*20)\n",
        "        \n",
        "        train_loss = 0\n",
        "        \n",
        "        resnet50.train()\n",
        "        \n",
        "        for train_step, (images, labels) in enumerate(dl_train):\n",
        "            optimizer.zero_grad()\n",
        "           # images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = resnet50(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            if train_step % 20 == 0:\n",
        "                print('Evaluating at step', train_step)\n",
        "                acc = 0\n",
        "                val_loss = 0\n",
        "                resnet50.eval()\n",
        "            \n",
        "                for val_step, (images, labels) in enumerate(dl_test):\n",
        "                    outputs = resnet50(images)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "                    val_loss +=loss.item()\n",
        "                \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    acc += sum((preds == labels).numpy())\n",
        "                val_loss /= (val_step +1)\n",
        "                acc = acc / len(test_dataset)\n",
        "                print(f'Val loss: {val_loss:.4f}, Acc: {acc:.4f}')\n",
        "                show_preds()\n",
        "                  \n",
        "                resnet50.train()\n",
        "                  \n",
        "                if acc > 0.96:\n",
        "                    print('Performance condition satisfied')\n",
        "  \n",
        "                    break\n",
        "        train_loss /= (train_step + 1)\n",
        "        print(f'Training loss: {train_loss:.4f}')\n",
        "    e=e+1\n",
        "    if e>3:\n",
        "      return        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PplD8O5faL3K"
      },
      "source": [
        "hist=train(epochs=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdGVLvJaaSCz"
      },
      "source": [
        "nb_classes = 3\n",
        "\n",
        "confusion_matrix3 = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dl_test):\n",
        "\n",
        "        resnet50.eval()\n",
        "        outputs = resnet50(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "                confusion_matrix3[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPGV6KAluEo"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix3(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    precision = confusion_matrix3[1,1] / sum(confusion_matrix3[:,1])\n",
        "    recall    = confusion_matrix3[1,1] / sum(confusion_matrix3[1,:])\n",
        "    f1_score  = 2*precision*recall / (precision + recall)\n",
        "    stats_text = \"\\n\\n\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(precision,recall,f1_score)\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' \n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label' + stats_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7wYU58oacJw"
      },
      "source": [
        "plot_confusion_matrix3(confusion_matrix3, train_dataset.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxP_c3RQLV84"
      },
      "source": [
        "model=resnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ_SRlBFPS_e"
      },
      "source": [
        "# **Save model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDLHKd-DIeRS"
      },
      "source": [
        "\n",
        "torch.save(model.state_dict(),\"/content/drive/My Drive/COVID-ResNext50_32x4d.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDGd6otSPhgA"
      },
      "source": [
        "### **Start Gridcam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1m0Aibah_M"
      },
      "source": [
        "import os\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from utils import visualize_cam, Normalize\n",
        "from gradcam import GradCAM, GradCAMpp\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84Nfyl8udfKK"
      },
      "source": [
        "\n",
        "#img_dir = 'images'\n",
        "img_name1 = cv2.imread('1.jpg',1)\n",
        "img_name2 = cv2.imread('2.png',1)\n",
        "img_name3 = cv2.imread('3.jpg',1)\n",
        "# img_name = 'multiple_dogs.jpg'\n",
        "# img_name = 'snake.JPEG'\n",
        "#img_name = 'water-bird.JPEG'\n",
        "#img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "#pil_img = PIL.Image.open(img_name)\n",
        "#pil_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOmSfCzfxmjd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoKvYanifUNV"
      },
      "source": [
        "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "torch_img = torch.from_numpy(np.asarray(img_name1)).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n",
        "torch_img = F.upsample(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "normed_torch_img = normalizer(torch_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTxkZ57lXExs"
      },
      "source": [
        "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "torch_img2 = torch.from_numpy(np.asarray(img_name2)).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n",
        "torch_img2 = F.upsample(torch_img2, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "normed_torch_img2 = normalizer(torch_img2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq-ZHxW-XPo0"
      },
      "source": [
        "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "torch_img3 = torch.from_numpy(np.asarray(img_name2)).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n",
        "torch_img3 = F.upsample(torch_img3, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "normed_torch_img3 = normalizer(torch_img3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03oIq0T45ndJ"
      },
      "source": [
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXDiT9f2wpT-"
      },
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet.eval(), resnet.cuda();\n",
        "\n",
        "\n",
        "\n",
        "cam_dict = dict()\n",
        "resnet_model_dict = dict(type='resnet50', arch=resnet, layer_name='layer4', input_size=(224, 224))\n",
        "resnet_gradcam = GradCAM(resnet_model_dict, True)\n",
        "resnet_gradcampp = GradCAMpp(resnet_model_dict, True)\n",
        "cam_dict['resnet'] = [resnet_gradcam, resnet_gradcampp]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x0tNq_SkkrR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7LGAJ6_klq2"
      },
      "source": [
        "images = []\n",
        "for gradcam, gradcam_pp in cam_dict.values():\n",
        "    mask, _ = gradcam(normed_torch_img)\n",
        "    mask=mask.cpu()\n",
        "    heatmap, result = visualize_cam(mask, torch_img)\n",
        "\n",
        "    mask_pp, _ = gradcam_pp(normed_torch_img)\n",
        "    mask_pp=mask_pp.cpu()\n",
        "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img)\n",
        "    \n",
        "    images.append(torch.stack([torch_img.squeeze().cpu(), heatmap, heatmap_pp, result, result_pp], 0))\n",
        "    \n",
        "images = make_grid(torch.cat(images, 0), nrow=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7XtEp8JXoBo"
      },
      "source": [
        "images2 = []\n",
        "for gradcam, gradcam_pp in cam_dict.values():\n",
        "    mask, _ = gradcam(normed_torch_img2)\n",
        "    mask=mask.cpu()\n",
        "    heatmap, result = visualize_cam(mask, torch_img2)\n",
        "\n",
        "    mask_pp, _ = gradcam_pp(normed_torch_img2)\n",
        "    mask_pp=mask_pp.cpu()\n",
        "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img2)\n",
        "    \n",
        "    images2.append(torch.stack([torch_img2.squeeze().cpu(), heatmap, heatmap_pp, result, result_pp], 0))\n",
        "    \n",
        "images2 = make_grid(torch.cat(images2, 0), nrow=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_rX8WSYX7Xj"
      },
      "source": [
        "plt.figure(figsize=(50,50))\n",
        "\n",
        "#for  i in range (0,2):\n",
        "plt.imshow(images2[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I6Tct8TSMyx"
      },
      "source": [
        "output_dir = 'outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_name = \"img_name\"\n",
        "output_path = os.path.join(output_dir, output_dir)\n",
        "\n",
        "#save_image(images, output_dir)\n",
        "#PIL.Image.open(output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlzoDYQGZBUI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_5hIRgFaUhQ"
      },
      "source": [
        "images3 = []\n",
        "for gradcam, gradcam_pp in cam_dict.values():\n",
        "    mask, _ = gradcam(normed_torch_img3)\n",
        "    mask=mask.cpu()\n",
        "    heatmap, result = visualize_cam(mask, torch_img3)\n",
        "\n",
        "    mask_pp, _ = gradcam_pp(normed_torch_img3)\n",
        "    mask_pp=mask_pp.cpu()\n",
        "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img3)\n",
        "    \n",
        "    images3.append(torch.stack([torch_img2.squeeze().cpu(), heatmap, heatmap_pp, result, result_pp], 0))\n",
        "    \n",
        "images3 = make_grid(torch.cat(images3, 0), nrow=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-r-dpZ-S08I"
      },
      "source": [
        "plt.figure(figsize=(50,50))\n",
        "\n",
        "\n",
        "plt.imshow(images3[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpkgMmwRdAr"
      },
      "source": [
        "plt.imshow(images[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJBekmhT-jnl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}